{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaging And Sorting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding The Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import exists\n",
    "from functools import reduce\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")#there are some warnings that show up from pandas that don't effect us, so we just mute them\n",
    "\n",
    "#authored by Tom Odem on 12 November 2023\n",
    "#computes the averages of data over a set increments of time for users, then merges averages with respective depression measurements\n",
    "def get_and_avg_data(avg_over_n_days = 7):\n",
    "#avg_over_n_days: integer value of the amount of days to compute averages over. defaults to 7, which computes weekly averages\n",
    "    \n",
    "    users_df = pd.read_csv('user_information.csv') #read the user_information.csv file to get user ids, depression scores, etc\n",
    "    n_days_df = pd.DataFrame(columns=['user_id','avg_step','avg_sleep','avg_drink', 'avg_eat','avg_care']) #initialize the dataframe that will hold averages over n days\n",
    "\n",
    "    #go through all users in user_information.csv\n",
    "    for user in users_df['user_id']:\n",
    "        if(exists('user_data/data_'+str(user)+'.csv')): #if the user's data csv exists then open it and continue\n",
    "            user_df = pd.read_csv('user_data/data_'+str(user)+'.csv')\n",
    "            \n",
    "            #find daily step count\n",
    "            user_df['client_time']= [pd.to_datetime(i).date() for i in user_df['client_time']]#turn the datetime entries into just dates\n",
    "            steps= user_df.groupby(['client_time'])['step'].max().reset_index().rename(columns={'client_time':'date'}).astype({'date':object})#compute the daily step count by just taking the maximum step count everyday, rename client_time to date so we can merge with others, force date to be object for merging\n",
    "            \n",
    "            if(exists('user_tags/'+str(user)+'.csv')): #if the user's tags csv exists then open it and continue\n",
    "                u = pd.read_csv('user_tags/'+str(user)+'.csv')\n",
    "                u = u.drop(columns=['end'])\n",
    "\n",
    "\n",
    "                #find daily sleep time\n",
    "                #finds the time the user wakes up everyday\n",
    "                wakeup_time = u.loc[(u['labelName'] == 'Wake up')]\n",
    "                wakeup_time['start'] = [pd.to_datetime(t)  for t in wakeup_time['start']]\n",
    "                wakeup_time['date']= [pd.to_datetime(t).date() for t in wakeup_time['start']]\n",
    "                wakeup_time['hour']= [pd.to_datetime(t).time() for t in wakeup_time['start']]\n",
    "\n",
    "                #finds the time the user went to sleep everyday\n",
    "                sleep_time = u.loc[(u['labelName'] == 'Sleep')]\n",
    "                sleep_time['start'] = [pd.to_datetime(t) for t in sleep_time['start']]\n",
    "                sleep_time['date']= [(pd.to_datetime(t)+ pd.Timedelta(days=1)).date() for t in sleep_time['start']]\n",
    "                sleep_time['hour']= [pd.to_datetime(t).time() for t in sleep_time['start']]\n",
    "                \n",
    "                #computes the amount of time the user slept daily\n",
    "                r = pd.merge(wakeup_time, sleep_time, on ='date')\n",
    "                r['start_y'] = pd.to_datetime(r['start_y'])\n",
    "                r['start_x'] = pd.to_datetime(r['start_x'])\n",
    "                r['sleeptime'] = (-1*(r['start_y'] - r['start_x']).astype('timedelta64[m]'))/60 #find the difference between when they woke up from when they went to sleep in hours\n",
    "                r = r[['sleeptime','date']].groupby('date').mean().reset_index().astype({'date':object})#we only need the date and the sleeptime, we rest the index to change it back \n",
    "                                                                                                        #to a dataframe, and we want to force teh date to be of type object so that we can always merge even if there are no entries\n",
    "                \n",
    "                #find daily number of times the user drank\n",
    "                drinktime = u.loc[(u['labelName'] == 'Drink')] #we only want the entries that correlate to drinking\n",
    "                drinktime['date'] = [pd.to_datetime(t).date() for t in drinktime['start']] #gives us the date that the drink happened, since we do not need to know the exact time\n",
    "                drinktime = drinktime.rename(columns={'labelName':'drinktime'}).groupby('date').count().drop(['start'], axis = 1).reset_index().astype({'date':object}) #finds the number of times the user drank a day by grouping by the date, we drop start becase\n",
    "                                                                                                                                                                        #we only need to know the date, we reset the index to turn it back into a dataframe, and we force date to be object for merging\n",
    "                \n",
    "                #find daily number of times the user ate\n",
    "                eattime = u.loc[(u['labelName'] == 'Eat')] #we only want the entries that correlate to eating\n",
    "                eattime['date'] = [pd.to_datetime(t).date() for t in eattime['start']] #gives us the date that the eat happened, since we do not need to know the exact time\n",
    "                eattime = eattime.rename(columns={'labelName':'eattime'}).groupby('date').count().drop(['start'], axis = 1).reset_index().astype({'date':object}) #finds the number of times the user ate a day by grouping by the date, we drop start becase\n",
    "                                                                                                                                                                #we only need to know the date, we reset the index to turn it back into a dataframe, and we force date to be object for merging\n",
    "                \n",
    "                #find daily number of times the user performed and act of self care\n",
    "                self_care = u.loc[(u['labelName'] == 'Take shower') | (u['labelName'] == 'Go to bathroom')] #we only want the entries that correlate to self care\n",
    "                self_care['date'] = [pd.to_datetime(t).date() for t in self_care['start']] #gives us the date that the self care happened, since we do not need to know the exact time\n",
    "                self_care = self_care.rename(columns={'labelName':'selfcare'}).groupby('date').count().drop(['start'], axis = 1).reset_index().astype({'date':object}) #finds the number of times the user self cared a day by grouping by the date, we drop start becase\n",
    "                                                                                                                                                                        #we only need to know the date, we reset the index to turn it back into a dataframe, and we force date to be object for merging\n",
    "\n",
    "                #merge all of the daily counts on time\n",
    "                data_frames = [r,drinktime,eattime,self_care,steps] #the dataframes to be merged\n",
    "                data_by_day_df = reduce(lambda  left,right: pd.merge(left,right,on=['date'],how='outer'), data_frames) #pd.merge can only merge two at a time, so we have to run merge over all of the dataframes\n",
    "\n",
    "                max_date = data_by_day_df['date'].max() #find the latest date in the dataframe\n",
    "\n",
    "                current_date = data_by_day_df['date'].min() #we start at the earliest date in the dataframe\n",
    "\n",
    "                \n",
    "                #take averages over avg_over_n_days incriments from the first day to the last day\n",
    "                while current_date < max_date: #while we haven't reached the last day\n",
    "                    n_days_from_current_date = current_date+datetime.timedelta(days=avg_over_n_days) #find the day that is avg_over_n_days away from the current date\n",
    "\n",
    "                    range = (data_by_day_df['date'] >= current_date) & (data_by_day_df['date'] < n_days_from_current_date) #define the range of dates we select from data_by_day_df \n",
    "\n",
    "                    #compute the averages of each activity within the given range\n",
    "                    avg_sleep = np.mean(data_by_day_df.loc[range]['sleeptime'])\n",
    "                    avg_drink = np.mean(data_by_day_df.loc[range]['drinktime'])\n",
    "                    avg_eat = np.mean(data_by_day_df.loc[range]['eattime'])\n",
    "                    avg_care = np.mean(data_by_day_df.loc[range]['selfcare'])\n",
    "                    avg_step = np.mean(data_by_day_df.loc[range]['step'])\n",
    "\n",
    "                    #add the averages to predic_df\n",
    "                    temp = pd.DataFrame([[user,avg_step, avg_sleep, avg_drink, avg_eat, avg_care]], columns=['user_id','avg_step','avg_sleep','avg_drink', 'avg_eat', 'avg_care'])\n",
    "                    n_days_df = pd.concat([n_days_df, temp])\n",
    "\n",
    "\n",
    "                    current_date = n_days_from_current_date #our range did not include the day avg_over_n_days away, so that day is now our current day to start from\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                \n",
    "        \n",
    "            else:\n",
    "                print(f'no user_tags: {user}') #the tags csv was missing for this user\n",
    "        else:\n",
    "            print(f'no user_data: {user}') #the data csv was missing for this user\n",
    "\n",
    "\n",
    "    averages_df = pd.merge(n_days_df, users_df[['user_id','depression_class', 'depression_score']], on='user_id').set_index('user_id') #merge the averages with their respective depression class and deppression score\n",
    "    return averages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no user_tags: 520\n",
      "no user_tags: 532\n",
      "no user_tags: 503\n",
      "no user_tags: 503\n",
      "no user_tags: 523\n",
      "no user_tags: 544\n",
      "no user_tags: 529\n",
      "no user_tags: 661\n",
      "no user_tags: 658\n",
      "no user_tags: 664\n",
      "no user_tags: 634\n",
      "no user_tags: 507\n",
      "no user_tags: 547\n",
      "no user_tags: 501\n",
      "no user_tags: 668\n",
      "no user_tags: 662\n",
      "1510\n"
     ]
    }
   ],
   "source": [
    "# example, getting averages over an increment of 5 days\n",
    "averages_df = get_and_avg_data(5)\n",
    "print(len(averages_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_step</th>\n",
       "      <th>avg_sleep</th>\n",
       "      <th>avg_drink</th>\n",
       "      <th>avg_eat</th>\n",
       "      <th>avg_care</th>\n",
       "      <th>depression_class</th>\n",
       "      <th>depression_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>691.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>3479.400000</td>\n",
       "      <td>21.923611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>3947.400000</td>\n",
       "      <td>22.597457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>4384.200000</td>\n",
       "      <td>22.111404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>2681.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>6417.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>8515.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>5010.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>2.50</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>4981.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.25</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>5550.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1510 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            avg_step  avg_sleep  avg_drink   avg_eat  avg_care  \\\n",
       "user_id                                                          \n",
       "519       691.000000        NaN        NaN  3.000000       NaN   \n",
       "519      3479.400000  21.923611        NaN  2.000000      1.00   \n",
       "519      3947.400000  22.597457        NaN  2.800000       NaN   \n",
       "519      4384.200000  22.111404        NaN  2.800000      1.00   \n",
       "519      2681.000000        NaN        NaN  2.333333      1.00   \n",
       "...              ...        ...        ...       ...       ...   \n",
       "655      6417.600000        NaN        NaN  1.800000      1.20   \n",
       "655      8515.333333        NaN        NaN  2.000000      2.00   \n",
       "655      5010.800000        NaN        NaN  1.750000      2.50   \n",
       "655      4981.200000        NaN        NaN  1.800000      1.25   \n",
       "655      5550.500000        NaN        NaN  2.500000      1.00   \n",
       "\n",
       "        depression_class  depression_score  \n",
       "user_id                                     \n",
       "519             Moderate               0.5  \n",
       "519             Moderate               0.5  \n",
       "519             Moderate               0.5  \n",
       "519             Moderate               0.5  \n",
       "519             Moderate               0.5  \n",
       "...                  ...               ...  \n",
       "655               Normal               0.0  \n",
       "655               Normal               0.0  \n",
       "655               Normal               0.0  \n",
       "655               Normal               0.0  \n",
       "655               Normal               0.0  \n",
       "\n",
       "[1510 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value for 'Normal' class: 0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "519     True\n",
       "519     True\n",
       "519     True\n",
       "519     True\n",
       "519     True\n",
       "       ...  \n",
       "655    False\n",
       "655    False\n",
       "655     True\n",
       "655     True\n",
       "655     True\n",
       "Name: avg_step, Length: 1510, dtype: bool"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display the dataframe\n",
    "#averages_df\n",
    "\n",
    "\n",
    "display(averages_df)\n",
    "max_value_normal = averages_df.loc[averages_df['depression_class'] == 'Normal', 'depression_score'].max()\n",
    "print(\"Max value for 'Normal' class:\", max_value_normal)\n",
    "\n",
    "#Make Nan values 0 to help with calculations later on\n",
    "averages_df['avg_sleep'].fillna(0, inplace=True)\n",
    "averages_df['avg_step'].fillna(0, inplace=True)\n",
    "\n",
    "display(averages_df['avg_step'] <= 6000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Depression Detection\n",
    "\n",
    "An SVM is utilized for detecting depression in users, considering three key factors:\n",
    "\n",
    "- **Sleep Duration:** If the user sleeps for *>= 10 hours a day* on average.\n",
    "\n",
    "- **Depression Score:** If the user has a depression score of *0.25 or higher*. This threshold is determined by finding the maximum depression score for a user with a level of 'Normal' depression.\n",
    "\n",
    "- **Average Daily Steps:** If the user averages *less than 6000 steps a day*. This criterion is based on a study conducted on Korean seniors. The dataset used in the study consisted of people on average 10 years younger, so the steps requirements were adjusted to accommodate the older people in our dataset ([source](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5976873/)).\n",
    "\n",
    "The SVM classifies a user as depressed if they pass the threshold with any **2 out of the 3 factors** listed above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "User with user_id 230.0 and avg_sleep 14.539612793119737 and depression_score 0.125 and avg_steps 9892.580465949824 is not classified as depressed.\n",
      "User with user_id 232.0 and avg_sleep 15.978037857216034 and depression_score 0.0 and avg_steps 3828.5887096774195 is classified as depressed.\n",
      "User with user_id 476.0 and avg_sleep 0.022727272727272728 and depression_score 0.0 and avg_steps 90.48863636363636 is classified as depressed.\n",
      "User with user_id 486.0 and avg_sleep 6.951515151515152 and depression_score 0.5 and avg_steps 10988.479924242425 is not classified as depressed.\n",
      "User with user_id 492.0 and avg_sleep 4.681818181818182 and depression_score 0.5 and avg_steps 4794.660227272727 is not classified as depressed.\n",
      "User with user_id 495.0 and avg_sleep 10.782719638242895 and depression_score 0.25 and avg_steps 2915.445348837209 is classified as depressed.\n",
      "User with user_id 496.0 and avg_sleep 5.842073170731707 and depression_score 0.75 and avg_steps 2404.8638211382117 is classified as depressed.\n",
      "User with user_id 499.0 and avg_sleep 0.0 and depression_score 0.0 and avg_steps 2232.320833333333 is not classified as depressed.\n",
      "User with user_id 504.0 and avg_sleep 5.384205426356589 and depression_score 0.5 and avg_steps 10327.26550387597 is not classified as depressed.\n",
      "User with user_id 505.0 and avg_sleep 9.588446969696971 and depression_score 0.5 and avg_steps 5006.642424242425 is not classified as depressed.\n",
      "User with user_id 508.0 and avg_sleep 2.635416666666667 and depression_score 0.75 and avg_steps 1130.2395833333335 is classified as depressed.\n",
      "User with user_id 512.0 and avg_sleep 0.0 and depression_score 0.875 and avg_steps 6082.359302325583 is not classified as depressed.\n",
      "User with user_id 516.0 and avg_sleep 1.0999999999999999 and depression_score 0.0 and avg_steps 760.411403508772 is classified as depressed.\n",
      "User with user_id 519.0 and avg_sleep 10.547135735192674 and depression_score 0.5 and avg_steps 1987.520720720721 is classified as depressed.\n",
      "User with user_id 527.0 and avg_sleep 0.0 and depression_score 0.0 and avg_steps 2180.9246913580246 is not classified as depressed.\n",
      "User with user_id 530.0 and avg_sleep 5.135227272727273 and depression_score 0.375 and avg_steps 3289.5162878787883 is classified as depressed.\n",
      "User with user_id 536.0 and avg_sleep 0.0 and depression_score 0.25 and avg_steps 3009.2897727272725 is not classified as depressed.\n",
      "User with user_id 540.0 and avg_sleep 0.0 and depression_score 0.0 and avg_steps 4418.535984848484 is not classified as depressed.\n",
      "User with user_id 552.0 and avg_sleep 0.0 and depression_score 1.0 and avg_steps 4321.612903225807 is not classified as depressed.\n",
      "User with user_id 569.0 and avg_sleep 0.29545454545454547 and depression_score 0.375 and avg_steps 3075.0803030303027 is not classified as depressed.\n",
      "User with user_id 572.0 and avg_sleep 0.0 and depression_score 0.5 and avg_steps 1920.9885714285715 is classified as depressed.\n",
      "User with user_id 574.0 and avg_sleep 9.689056637021281 and depression_score 0.0 and avg_steps 3857.6079545454545 is not classified as depressed.\n",
      "User with user_id 580.0 and avg_sleep 5.387927350427351 and depression_score 0.125 and avg_steps 1095.9871794871794 is classified as depressed.\n",
      "User with user_id 582.0 and avg_sleep 2.602272727272727 and depression_score 0.375 and avg_steps 4155.108333333334 is not classified as depressed.\n",
      "User with user_id 583.0 and avg_sleep 0.0 and depression_score 0.25 and avg_steps 13.227272727272727 is classified as depressed.\n",
      "User with user_id 585.0 and avg_sleep 6.144791666666667 and depression_score 0.875 and avg_steps 3292.0308333333332 is classified as depressed.\n",
      "User with user_id 586.0 and avg_sleep 0.0 and depression_score 0.25 and avg_steps 1346.2261904761906 is classified as depressed.\n",
      "User with user_id 635.0 and avg_sleep 0.0 and depression_score 0.0 and avg_steps 10019.624761904764 is not classified as depressed.\n",
      "User with user_id 644.0 and avg_sleep 1.8409090909090908 and depression_score 0.0 and avg_steps 5511.169696969696 is not classified as depressed.\n",
      "User with user_id 650.0 and avg_sleep 8.819761904761904 and depression_score 0.125 and avg_steps 5107.0871428571445 is not classified as depressed.\n",
      "User with user_id 651.0 and avg_sleep 2.1614583333333335 and depression_score 0.375 and avg_steps 1533.7302083333334 is classified as depressed.\n",
      "User with user_id 653.0 and avg_sleep 9.402380952380952 and depression_score 0.5 and avg_steps 15838.285714285714 is not classified as depressed.\n",
      "User with user_id 655.0 and avg_sleep 13.841190476190476 and depression_score 0.0 and avg_steps 5270.43380952381 is not classified as depressed.\n",
      "User with user_id 656.0 and avg_sleep 0.0 and depression_score 1.0 and avg_steps 1761.7950980392156 is classified as depressed.\n",
      "User with user_id 665.0 and avg_sleep 0.0 and depression_score 0.0 and avg_steps 9236.364215686273 is not classified as depressed.\n",
      "User with user_id 672.0 and avg_sleep 0.0 and depression_score 0.625 and avg_steps 4545.937931034481 is not classified as depressed.\n",
      "User with user_id 674.0 and avg_sleep 0.5882352941176471 and depression_score 0.0 and avg_steps 1539.627450980392 is classified as depressed.\n",
      "This is the number of users that are depressed:  16\n",
      "This is the number of users that are not depressed:  21\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Combine the user id's to include only unique ones\n",
    "unique_users_df = averages_df[['avg_sleep', 'depression_score', 'avg_step']].groupby('user_id').mean().reset_index()\n",
    "\n",
    "# Creating the three features for the SVM\n",
    "avg_sleep_values = unique_users_df['avg_sleep'].values\n",
    "depression_score_values = unique_users_df['depression_score'].values\n",
    "avg_step_values = unique_users_df['avg_step'].values\n",
    "\n",
    "# Define thresholds for sleep duration and depression score and average steps\n",
    "sleep_threshold = 10.0\n",
    "depression_score_threshold = 0.25\n",
    "steps_threshold = 6000\n",
    "\n",
    "# Count the number of conditions met for each user\n",
    "conditions_met = ((avg_sleep_values >= sleep_threshold).astype(int) +\n",
    "                  (depression_score_values >= depression_score_threshold).astype(int) +\n",
    "                  (avg_step_values <= steps_threshold).astype(int))\n",
    "\n",
    "# Classify a user as depressed if at least two out of three conditions are met\n",
    "labels = (conditions_met >= 2).astype(int)\n",
    "\n",
    "# Reshape the data to a 2D array \n",
    "features = np.column_stack((avg_sleep_values, depression_score_values, avg_step_values))\n",
    "\n",
    "# Create an SVM classifier\n",
    "classifier = svm.SVC(kernel='linear')\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(features, labels)\n",
    "\n",
    "# Make predictions for the training data\n",
    "predictions = classifier.predict(features)\n",
    "\n",
    "# Display the predictions\n",
    "predictions_df = pd.DataFrame({'user_id': unique_users_df['user_id'], 'avg_sleep': avg_sleep_values, 'depression_score': depression_score_values,'avg_step': avg_step_values, 'depression_prediction': predictions})\n",
    "depressed_count = 0\n",
    "n_depressed_count = 0\n",
    "# Print whether each user is depressed or not\n",
    "print(len(predictions_df))\n",
    "for index, row in predictions_df.iterrows():\n",
    "    if row['depression_prediction'] == 1:\n",
    "        print(f\"User with user_id {row['user_id']} and avg_sleep {row['avg_sleep']} and depression_score {row['depression_score']} and avg_steps {row['avg_step']} is classified as depressed.\")\n",
    "        depressed_count += 1\n",
    "    else:\n",
    "        print(f\"User with user_id {row['user_id']} and avg_sleep {row['avg_sleep']} and depression_score {row['depression_score']} and avg_steps {row['avg_step']} is not classified as depressed.\")\n",
    "        n_depressed_count += 1\n",
    "print(\"This is the number of users that are depressed: \", depressed_count)\n",
    "print(\"This is the number of users that are not depressed: \", n_depressed_count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
