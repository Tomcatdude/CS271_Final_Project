{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LSTMs To Find If A User Will Have Higher Or Lower Than Average Amount Of Sleep Based On Their Daily Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guessing sleep amounts based on daily activity is apparently not a popular topic for LSTM architectures, so we had to be creative with our research. What we realized after some digging is that the problem of finding patterns in daily activities is a lot like finding patterns in writing. Earlier actions influence actions that come later and vice versa, just like words in sentences and paragraphs. Our LSTM architecture is based on the model found in the article written by Li, where they used an LSTM to categorize customer complaints into different product categories (Credit Card, Mortgage, Loans, etc). With some modifications to our data and the LSTM model described in the article, we were able to train an LSTM to predict with 83.9% accuracy if a user is more likely to sleep more than or equal to the average sleep duration or if they would sleep less than the average.\n",
    "\n",
    "Li, S. (2019, April 10). Multi-class text classification with LSTM. Medium. https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data and turn it into data usable by LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no user_tags: 520\n",
      "no user_tags: 532\n",
      "no user_tags: 503\n",
      "no user_tags: 503\n",
      "no user_tags: 523\n",
      "no user_tags: 544\n",
      "no user_tags: 529\n",
      "no user_tags: 661\n",
      "no user_tags: 658\n",
      "no user_tags: 664\n",
      "no user_tags: 634\n",
      "no user_tags: 507\n",
      "no user_tags: 547\n",
      "no user_tags: 501\n",
      "no user_tags: 668\n",
      "no user_tags: 662\n"
     ]
    }
   ],
   "source": [
    "#Get data and then split it into sequences (X) and class (Y)\n",
    "X = modules.get_and_dayitise_data()#read in data from csv and turn into day by day sequences\n",
    "Y = np.array([x[0] for x in X]) #get the sleep times (1st column)\n",
    "X = np.array([x[1:] for x in X]) #remove the sleep times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is obtained via two different file models: user_information.csv, which lists every user who participated in the study, and user_tag files, which are individual files for each user that documents every action that was recorded throughout the study. The function get_and_dayitise_data() opens every user_tag file for each user, computes the amount of sleep the user had each day, and then groups daily actions together with their respective daily sleep amounts. What is returned is an array of arrays, where each subarray holds a single day's sleep amount followed by the actions performed during that day. The amount of days held in the array is equal to the amount of days where a daily sleep amount was obtained for every day for every user. Since we are trying to guess the amount of sleep based on daily activities, we take the first column of this array as our labels and everything proceeding as our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get only the activity sequences that have more than some number of activities per day\n",
    "lowest_num_activities = 3\n",
    "Y = np.array([Y[i] for i in range(len(Y)) if len(X[i])>lowest_num_activities]) #update the Y to only include indices who's X length is greater than lowest_sum_activities\n",
    "X = [x for x in X if len(x)>lowest_num_activities] #update X with only those indices whos number of activities is greater than lowest_num_activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some days within our data just didn't have a lot of activities recorded, which is probably the fault of the students who labeled the data, rather than users actually performing less actions. Because of this, we designated any days that contained less than or equal to 3 activities to be outliers, and so removed them. We chose the number 3 because we believe that 4 or more activities can have a significant impact on a user's day. This left us with SOME NUMBER of data points to train and test with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode Y to have 2 classes (less than average sleep or more than or equal average sleep)\n",
    "\n",
    "#put Y values into 2 classes\n",
    "average_sleep = np.mean(Y) #get the average amount of sleep\n",
    "Y[Y < average_sleep] = 0 #less than average\n",
    "Y[Y >= average_sleep] = 1 #more than or equal average\n",
    "\n",
    "#one hot encode it so we have a larger dimensionality that will match the output of the LSTM\n",
    "onehot_encoder = OneHotEncoder(categories='auto')\n",
    "Y = onehot_encoder.fit_transform(Y.reshape(-1,1)).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our LSTM model uses a softmax function to obtain a probability for multiple classes, we need to one hot encode the labels to be in multiple classes so that the LSTM can use them. Todo this we simply found the average amount of sleep within our labels and classified each label as 1 (more or equal to the average amount of sleep) or 0 (less than the average amount of sleep). Once our label array was a 1 dimensional array of 1s and 0s, we can now one hot encode it into a 2D array with two columns, each column representing 1 or 0. Our labels were now ready for the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the X labels into numeric values. important note: each row is not the same length\n",
    "le = LabelEncoder()\n",
    "le.fit_transform(np.hstack(X)) #flatten X so we can fit the encoder to all possible values in each row\n",
    "X = np.array([le.transform(x)+1 for x in X]) #now transform each row into corresponding encoding using the fitted encoder, add 1 so that we don't have any zeroes because 0 is for padding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the labels, we had to encode the daily actions into integer representations so that the LSTM could read them. Since each action was one of a handful of set string values, we simply used the LabelEncoder object from sklearn to encode each string to an integer. We add 1 to each value to ensure that there is no 0 category, since the value of 0 will be used to pad the data in the next step and so signifies a \"none\" activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1488, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#resize the arrays to be of size average length\n",
    "average_length = int(np.median([len(x) for x in X])) #get the average length\n",
    "#if the length is less than the average, pad it with zeroes. if the length is over, resize it\n",
    "X = np.array([np.resize(np.pad(x,(0,average_length-len(x))), average_length) if len(x) < average_length else np.resize(x,average_length) for x in X])\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be used as input in our LSTM, each day had to be the same length. Unfortunately, each day in our dataset is of a different and random length determined by user actions. To create equal length input we first found the average length of all days. This is the length we want every day to be. So, we either padded days with less than the average-amount-of-actions with zeroes or took the first average-amount-of-actions actions from days with more than average-amount-of-actions actions. This gives us a dataset where each day is average-amount-of-actions in length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split up our data into trains and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "76/76 [==============================] - 3s 12ms/step - loss: 0.3822 - accuracy: 0.8622 - val_loss: 0.3456 - val_accuracy: 0.8881\n",
      "Epoch 2/5\n",
      "76/76 [==============================] - 1s 8ms/step - loss: 0.3383 - accuracy: 0.8863 - val_loss: 0.3279 - val_accuracy: 0.8955\n",
      "Epoch 3/5\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3282 - accuracy: 0.8888 - val_loss: 0.3610 - val_accuracy: 0.8657\n",
      "Epoch 4/5\n",
      "76/76 [==============================] - 1s 8ms/step - loss: 0.3147 - accuracy: 0.8880 - val_loss: 0.3620 - val_accuracy: 0.8955\n",
      "Epoch 5/5\n",
      "60/76 [======================>.......] - ETA: 0s - loss: 0.2970 - accuracy: 0.8979"
     ]
    }
   ],
   "source": [
    "#test multiple models out and find which is the best\n",
    "\n",
    "embedding = 100 #helps with finding patterns in daily routines\n",
    "num_unique_labels = len(le.classes_)+1\n",
    "                #learning rate, epochs, dropout, batch_size, accuracy\n",
    "best_results = [0.0,            0,      0.0,     0,          0.0] #saves the best results found throughout testing\n",
    "#these are the various model parameters we will be trying\n",
    "learning_rates = [.01,.02,.03,.05,.06,.07,.08,.09,.1]\n",
    "epoch_rates = [5,10,15]\n",
    "dropout_rates = [.01,.02,.03,.05,.06,.07,.08,.09,.1]\n",
    "batch_sizes = [16,32,64]\n",
    "\n",
    "for lr in learning_rates: #these loops try every combination of the above paramters sets, and will populate best_results \n",
    "    for epochs in epoch_rates:#with the paramaters that returns the best accuracy\n",
    "        for dropout in dropout_rates:\n",
    "            for batch_size in batch_sizes: \n",
    "                model = Sequential() #initialize the model\n",
    "                model.add(Embedding(num_unique_labels, embedding, input_length=average_length)) #first layer is embedding, it will try to learn the daily activity patterns\n",
    "                model.add(SpatialDropout1D(dropout)) #drop out feature maps\n",
    "                model.add(LSTM(100, dropout=dropout, recurrent_dropout=dropout)) #LSTM layer\n",
    "                model.add(Dense(2, activation='softmax')) #using soft_max for possibility of multiple classes\n",
    "                adam = optimizers.Adam(learning_rate=lr) #using Adam optimizer\n",
    "                model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy']) #again using categorical_crossentropy for possibility of multiple classes\n",
    "\n",
    "                #actually train the model, use a validation split of .1 and also allow the ability to early stop\n",
    "                history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "                #evaluate the model we just trained and save it if it is better\n",
    "                accuracy = model.evaluate(X_test,Y_test)\n",
    "                if(accuracy[1] > best_results[4]): #the accuracy is the best we have seen so far, so save it\n",
    "                    best_results = [lr, epochs, dropout, batch_size, accuracy[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the best model that was found above and print it's model summary\n",
    "\n",
    "#get the best paramaters\n",
    "lr = best_results[0]\n",
    "epochs = best_results[1]\n",
    "dropout = best_results[2]\n",
    "batch_size = best_results[3]\n",
    "\n",
    "#define the model using the best parameters\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_unique_labels, embedding, input_length=average_length))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "adam = optimizers.Adam( learning_rate=lr)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "19/19 [==============================] - 3s 29ms/step - loss: 0.7408 - accuracy: 0.6614 - val_loss: 0.5881 - val_accuracy: 0.6866\n",
      "Epoch 2/5\n",
      "19/19 [==============================] - 0s 15ms/step - loss: 0.4815 - accuracy: 0.7759 - val_loss: 0.5688 - val_accuracy: 0.7015\n",
      "Epoch 3/5\n",
      "19/19 [==============================] - 0s 15ms/step - loss: 0.4454 - accuracy: 0.7934 - val_loss: 0.4931 - val_accuracy: 0.7463\n",
      "Epoch 4/5\n",
      "19/19 [==============================] - 0s 15ms/step - loss: 0.4351 - accuracy: 0.7959 - val_loss: 0.5200 - val_accuracy: 0.7537\n",
      "Epoch 5/5\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4529 - accuracy: 0.7851 - val_loss: 0.4991 - val_accuracy: 0.7985\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3991 - accuracy: 0.8389\n",
      "Test set\n",
      "  Loss: 0.399\n",
      "  Accuracy: 0.839\n"
     ]
    }
   ],
   "source": [
    "#train the best model and print it's accuracy\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "accuracy = model.evaluate(X_test,Y_test)\n",
    "print(f'Test set\\n  Loss: {accuracy[0]}\\n  Accuracy: {accuracy[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
